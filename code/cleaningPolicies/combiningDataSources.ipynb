{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30023172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "#Load the row paragraphs\n",
    "with open('labeledData.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "    \n",
    "#Load the labels\n",
    "labels = pd.read_excel('labels.xls', sheet_name = None)\n",
    "\n",
    "del labels['TEMPLATE SHEET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eb7b5b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading megaresistencia.com\n",
      "---- megaresistencia.com.md found in data source\n",
      "Loading themcrookedvultures.com\n",
      "---- themcrookedvultures.com.md found in data source\n",
      "Loading icanhascheezburger.com\n",
      "---- icanhascheezburger.com.md found in data source\n",
      "Loading perezitos.com\n",
      "---- perezitos.com.md found in data source\n",
      "Loading statdx.com\n",
      "---- statdx.com.md found in data source\n"
     ]
    }
   ],
   "source": [
    "for filename in labels.keys():\n",
    "    print('Loading '+filename)\n",
    "    if filename+'.md' in data.keys():\n",
    "        print('---- '+filename+'.md'+\" found in data source\")\n",
    "        for paraTuple in data[filename+'.md']:\n",
    "            paraIndex = paraTuple[0]\n",
    "            paragraph = paraTuple[1]\n",
    "            paragraphProcesses = \" \".join(list(filter(lambda a: a != '\\n', paragraph)))        \n",
    "            labels[filename].loc[paraIndex, 'PARAGRAPH'] = paragraphProcesses\n",
    "        labels[filename]['PARAGRAPH'] = labels[filename]['PARAGRAPH'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7e79663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DB.pickle', 'wb') as handle:\n",
    "    pickle.dump(labels, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a9c9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"How HugeDomains Collects this Information HugeDomains utilizes a variety of security measures to maintain the safety of customers' personal information. All supplied financial information (including credit card data) is transmitted via Secure Socket Layer (SSL) technology and then encrypted into the company's payment gateway provider's database. This database and the sensitive information contained within are only accessible by those authorized with special access rights to such systems and are required to keep the information confidential. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ea2f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/patrickzoechbauer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt') # if necessary...\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "83d70b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bestfit(example, labels):\n",
    "    bestfit = dict() \n",
    "    bestfit['similarity'] = 0\n",
    "    for file in labels: \n",
    "        for para in range(len(labels[file])):\n",
    "            similarity = cosine_sim(example, labels[file].loc[para,'PARAGRAPH'])        \n",
    "            if bestfit['similarity'] < similarity:\n",
    "                #new best fit\n",
    "                bestfit['similarity'] = similarity\n",
    "                bestfit['file'] = file\n",
    "                bestfit['label'] = labels[file].loc[para, 'LABEL']\n",
    "                bestfit['PARAGRAPH'] = labels[file].loc[para, 'PARAGRAPH']\n",
    "    return bestfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a245219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
